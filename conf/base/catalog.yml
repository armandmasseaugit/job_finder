# xlsx file containing all useful information on all recently scraped jobs
# (company name, offer publication date, job title etc.)
wttj_jobs_output:
  type: pandas.ExcelDataset
  filepath: "abfs://wttj-scraping/wttj_jobs.xlsx"
  credentials: azure_credentials
  load_args:
    engine: openpyxl

# JSON file containing the date of the last scrape
wttj_last_scrape:
  type: json.JSONDataset
  filepath: "abfs://wttj-scraping/wttj_last_scrape.json"
  credentials: azure_credentials

# JSON file containing all the references of the jobs from wttj_jobs.xlsx file
# that were liked or disliked. Its structure is {"ref of the offer 1": "like",
# "ref of the offer 2": "dislike" ...}.
job_likes:
  type: json.JSONDataset
  filepath: "abfs://liked-jobs/job_likes.json"
  credentials: azure_credentials

# Previous model
rl_model_old:
  type: pickle.PickleDataset #job_finder.datasets.OptionalPickleDataset
  filepath: "abfs://wttj-scraping/rl_model.pkl"
  versioned: true
  credentials: azure_credentials

# New model that has been retrained with job_likes and wttj_jobs data
rl_model_new:
  type: pickle.PickleDataset #job_finder.datasets.OptionalPickleDataset
  filepath: "abfs://wttj-scraping/rl_model.pkl"
  versioned: true
  credentials: azure_credentials

# JSON file containing the note (called the relevance score and it a note from 0 to 1)
# of all the jobs that were scraped. Its structure is: {"ref of the offer 1": 0.002,
# "ref of the offer 2": 0.765 ...}.
scored_offers:
  type: json.JSONDataset
  filepath: "abfs://relevance-scores/scored_jobs.json"
  credentials: azure_credentials

# ChromaDB vector database for job embeddings and semantic search
jobs_vector_db:
  type: job_finder.datasets.ChromaDataset
  collection_name: "jobs"
  persist_directory: "./data/chroma"
  embedding_model: "paraphrase-multilingual-MiniLM-L12-v2"
